# Multimodal Aphasia Analysis Viewer

A Streamlit-based web application for reviewing and analyzing multimodal speech and gesture data from patients with aphasia. The tool visualizes video segments, gesture keyframes, transcripts, and model outputs side by side — designed to help clinicians and researchers evaluate communication patterns and ASR accuracy.

---

## What It Does

Aphasia affects a person's ability to speak, understand, read, and write. Studying it requires analyzing both speech (transcripts) and non-verbal cues (gestures) together. This app loads a structured JSON dataset — output from a multimodal analysis pipeline — and provides an interactive interface to:

- Review video segments alongside gesture grid images for each task
- Compare ground truth CHA transcripts against ASR-generated transcripts
- Calculate Word Error Rate (WER) at the segment level and overall per participant
- View augmented transcripts with gesture markers inserted inline
- Browse results by participant and task with sidebar navigation

---

## Project Structure

- `visualizer.py` — Main Streamlit application
- `style.css` — Custom styling for the UI
- `requirements.txt` — Python dependencies

---

## How It Works

### Data Input
The app takes a JSON file as input — each entry in the JSON represents one task segment for one participant. It expects the following folder structure alongside the JSON:

```
your_data_folder/
├── analysis.json
├── videos/
│   ├── participant1.mp4
│   └── ...
└── gesture_grids/
    ├── participant1/
    │   ├── grid_001.png
    │   └── ...
    └── participant2/
        └── ...
```

### Transcript Comparison & WER
The app compares two transcripts for each segment:
- **CHA transcript** — manually annotated ground truth
- **ASR transcript** — automatically generated by a speech recognition system

Word Error Rate (WER) is calculated using the `jiwer` library after preprocessing — lowercasing, removing speaker labels, stripping punctuation, and normalizing whitespace. WER is shown both per segment and as an overall score per participant (computed by concatenating all their segments).

### Gesture & Keyframe Display
Gesture grid images are loaded from `gesture_grids/participant_id/` and displayed alongside the video. If LLM-selected keyframes are present in the JSON, those are shown with priority. Otherwise, the pipeline-generated gesture motion sequence grids are displayed.

### LLM Analysis Output
If the JSON contains model output (from a Qwen-VL or similar multimodal model), the app parses and displays:
- Augmented transcript — original ASR with gesture markers inserted
- Model explanation — the model's reasoning about gesture-speech alignment

If no LLM output is present, it falls back to showing the raw pipeline augmented transcript and ASR text.

---

## Running the App

Install dependencies:

```bash
pip install -r requirements.txt
pip install streamlit
```

Run the app:

```bash
streamlit run visualizer.py
```

Then upload your JSON file from the sidebar. Make sure your `videos/` and `gesture_grids/` folders are in the same directory where you run the command.

---

## Tech Stack

- **Streamlit** — interactive web UI
- **Pandas, NumPy** — data handling and metrics
- **jiwer** — Word Error Rate calculation
- **Pillow** — image loading for gesture grids
- **unidecode** — Unicode normalization for WER preprocessing

---

## Dataset Format

Each entry in the JSON should contain fields like:
- `participant_identifier` — unique patient ID
- `task_segment_id` — unique segment ID
- `task_label` — type of communication task
- `task_start_time_sec`, `task_end_time_sec` — video timestamps
- `concatenated_filtered_cha_transcript` — ground truth transcript
- `speaker_labeled_asr_transcript` — ASR output
- `gesture_motion_sequence_grid_image_paths` — list of gesture grid image paths
- `model_augmented_transcript`, `model_explanation` — LLM output (optional)

> **Note:** Dataset files are not included in this repository due to patient privacy considerations.
